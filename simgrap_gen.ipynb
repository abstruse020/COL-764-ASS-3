{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdf86ea9-f7ae-4d9e-9ea9-cbc2bac5dc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import sys\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4b99f42-a63c-4520-ab2f-5ccd8f8fe6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Vocab_class import TermVocab\n",
    "TVocab = TermVocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6884ba82-8ad1-4680-893f-011a5926d587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/harsh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aace6e8c-e3a7-4c4d-bd6b-ff6aee6ee80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "coll_dir = '20news-bydate-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e487c1c5-a06f-45c7-8587-6efc8e18dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class genSimGraph:\n",
    "    def __init__(self, coll_dir, op_file):\n",
    "        self.coll_dir = coll_dir\n",
    "        self.op_file = op_file\n",
    "        self.doc_term_dict = None\n",
    "        self.vocab = TermVocab()\n",
    "        self.limit_file_read_to = 10\n",
    "        self.file_count = 0\n",
    "        \n",
    "    def generate_term_set(self):\n",
    "        limit_file_read_to = self.limit_file_read_to\n",
    "        coll_dir = self.coll_dir\n",
    "        doc_term_dict = {}\n",
    "        print('Creating Term set for docs')\n",
    "        ps = PorterStemmer()\n",
    "        file_count = 0\n",
    "        read_file_count = 0\n",
    "        for foldername in os.listdir(coll_dir):\n",
    "            print('For Folder:', foldername)\n",
    "            path_to_file = os.path.join(coll_dir, foldername)\n",
    "            for filename in os.listdir(path_to_file):\n",
    "                file_count+=1\n",
    "                #print('---------------------For File:', filename)\n",
    "                print('file:', foldername+'/'+filename,end = ' ')\n",
    "                with open(os.path.join(path_to_file, filename), 'r') as f:\n",
    "                    content = None\n",
    "                    try:\n",
    "                        content = f.read()\n",
    "                    except:\n",
    "                        print('cant read',foldername +'/'+filename)\n",
    "                    read_file_count +=1\n",
    "                    #print(content)\n",
    "                    doc_terms = word_tokenize(content)\n",
    "                    #print(doc_terms)\n",
    "                    stemmed_terms = [ps.stem(term) for term in doc_terms]\n",
    "                    #print('non unique',len(stemmed_terms))\n",
    "                    #print('uniq', len(set(stemmed_terms)))\n",
    "                    doc_term_dict[foldername+'/'+filename] = set(stemmed_terms)\n",
    "                    [self.vocab.add_term(term) for term in stemmed_terms]\n",
    "                    #TVocab.add_term(stemmed_terms[0])\n",
    "                    #print(stemmed_terms)\n",
    "                if file_count>=limit_file_read_to:\n",
    "                    break\n",
    "            if file_count>=limit_file_read_to:\n",
    "                break\n",
    "        print(file_count)\n",
    "        self.file_count = file_count\n",
    "        print('Read File Count:', read_file_count)\n",
    "        self.doc_term_dict = doc_term_dict\n",
    "        return doc_term_dict\n",
    "    \n",
    "    def write_to_file(self, doc1, doc2, sim, path = None):\n",
    "        if path == None:\n",
    "            path = self.op_file\n",
    "        content = doc1 + '\\t' + doc2 + '\\t' + str(sim) + '\\n'\n",
    "        with open(path, '+a') as file:\n",
    "            file.write(content)\n",
    "        return True\n",
    "    \n",
    "    def clear_file(self, filename):\n",
    "        open(filename, 'w').close()\n",
    "        return True\n",
    "    \n",
    "    def gen_vocab(self):\n",
    "        limit_file_read_to = self.limit_file_read_to\n",
    "        coll_dir = self.coll_dir\n",
    "        idf = {}\n",
    "        self.vocab = TermVocab()\n",
    "        print('Generating vocab')\n",
    "        ps = PorterStemmer()\n",
    "        file_count = 0\n",
    "        read_file_count = 0\n",
    "        for foldername in os.listdir(coll_dir):\n",
    "            print('For Folder:', foldername)\n",
    "            path_to_file = os.path.join(coll_dir, foldername)\n",
    "            for filename in os.listdir(path_to_file):\n",
    "                file_count+=1\n",
    "                #print('---------------------For File:', filename)\n",
    "                print('file:', foldername+'/'+filename,end = ' ')\n",
    "                with open(os.path.join(path_to_file, filename), 'r') as f:\n",
    "                    content = None\n",
    "                    try:\n",
    "                        content = f.read()\n",
    "                    except:\n",
    "                        print('cant read',foldername +'/'+filename)\n",
    "                    \n",
    "                    read_file_count +=1\n",
    "                    #print(content)\n",
    "                    doc_terms = word_tokenize(content)\n",
    "                    #print(doc_terms)\n",
    "                    stemmed_terms = set([ps.stem(term) for term in doc_terms])\n",
    "                    doc_term_dict[foldername+'/'+filename] = set(stemmed_terms)\n",
    "                    for term in stemmed_terms:\n",
    "                        self.vocab.add_term(term)\n",
    "                        if term in idf:\n",
    "                            idf[term] +=1\n",
    "                        else:\n",
    "                            idf[term] = 1\n",
    "                    #TVocab.add_term(stemmed_terms[0])\n",
    "                    #print(stemmed_terms)\n",
    "                if file_count>=limit_file_read_to:\n",
    "                    break\n",
    "            if file_count>=limit_file_read_to:\n",
    "                break\n",
    "        D = file_count\n",
    "        for term in idf:\n",
    "            idf[term] = np.log(1 +D/idf[term]) \n",
    "        print(file_count)\n",
    "        self.file_count = file_count\n",
    "        return idf\n",
    "    \n",
    "    def make_doc_vectors(self):\n",
    "        limit_file_read_to = self.limit_file_read_to\n",
    "        coll_dir = self.coll_dir\n",
    "        doc_term_dict = {}\n",
    "        tf = {}\n",
    "        idf = self.gen_vocab()\n",
    "        n = self.vocab.vocab_length\n",
    "        print('Creating vector for docs')\n",
    "        ps = PorterStemmer()\n",
    "        file_count = 0\n",
    "        read_file_count = 0\n",
    "        for foldername in os.listdir(coll_dir):\n",
    "            print('For Folder:', foldername)\n",
    "            path_to_file = os.path.join(coll_dir, foldername)\n",
    "            for filename in os.listdir(path_to_file):\n",
    "                file_count+=1\n",
    "                #print('---------------------For File:', filename)\n",
    "                print('file:', foldername+'/'+filename,end = ' ')\n",
    "                with open(os.path.join(path_to_file, filename), 'r') as f:\n",
    "                    content = f.read()\n",
    "                    read_file_count +=1\n",
    "                    \n",
    "                    #tokenize and stem\n",
    "                    doc_terms = word_tokenize(content)\n",
    "                    if len(doc_terms) < 1:\n",
    "                        continue\n",
    "                    stemmed_terms = [ps.stem(term) for term in doc_terms]\n",
    "                    \n",
    "                    #make tf vector for docs\n",
    "                    doc_v = np.zeros(n)\n",
    "                    for term in stemmed_terms:\n",
    "                        doc_v[self.vocab.to_index(term)] += 1\n",
    "                    doc_v = np.ma.log(doc_v)\n",
    "                    tf[foldername+'/'+filename] = 1 + doc_v.filled(0)\n",
    "                    \n",
    "                    #include idf part in the vector\n",
    "                    for term_index in range(self.vocab.vocab_length):\n",
    "                        term = self.vocab.to_term(term_index)\n",
    "                        tf[foldername+'/'+filename][term_index] *= idf[term]\n",
    "                    \n",
    "                if file_count>=limit_file_read_to:\n",
    "                    break\n",
    "            if file_count>=limit_file_read_to:\n",
    "                break\n",
    "        print(file_count)\n",
    "        return tf\n",
    "        \n",
    "    def jaccard_sim(self):\n",
    "        doc_term_dict = self.doc_term_dict\n",
    "        self.clear_file(self.op_file)\n",
    "        n = len(doc_term_dict)\n",
    "        jacob_mat = np.zeros((n,n))\n",
    "        for i, itr_i in zip(doc_term_dict, range(n)):\n",
    "            for j, itr_j in zip(doc_term_dict, range(n)):\n",
    "                if itr_j <= itr_i:\n",
    "                    continue\n",
    "                term_dic_i = doc_term_dict[i]\n",
    "                term_dic_j = doc_term_dict[j]\n",
    "                if len(term_dic_i)>0 or len(term_dic_j)>0:\n",
    "                    nr = len(term_dic_i & term_dic_j)\n",
    "                    dr = len(term_dic_i | term_dic_j)\n",
    "                    jacob_mat[itr_i, itr_j] = nr/dr\n",
    "                    if jacob_mat[itr_i, itr_j] != 0:\n",
    "                        self.write_to_file(i, j, jacob_mat[itr_i, itr_j])\n",
    "                itr_j +=1\n",
    "            itr_i +=1\n",
    "        return jacob_mat\n",
    "    \n",
    "    def cosine_sim(self, d1, d2):\n",
    "        sim = np.dot(d1, d2)/(np.linalg.norm(d1) * np.linalg.norm(d2))\n",
    "        return sim\n",
    "    \n",
    "    def tf_idf_sim(self):\n",
    "        doc_vectors = self.make_doc_vectors()\n",
    "        n = self.vocab.vocab_length\n",
    "        m = self.file_count\n",
    "        print('file count:', m)\n",
    "        sim_mat = np.zeros((m,m))\n",
    "        for d1, i in zip(doc_vectors, range(m)):\n",
    "            for d2, j in zip(doc_vectors, range(m)):\n",
    "                if j <= i:\n",
    "                    continue\n",
    "                sim = self.cosine_sim(doc_vectors[d1], doc_vectors[d2])\n",
    "                sim_mat[i,j] = sim\n",
    "                if sim > 0:\n",
    "                    self.write_to_file(d1, d2, sim, 'tf_idf_sim.txt')\n",
    "        print(sim_mat)\n",
    "        return True\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e77bdd57-10df-4a78-af30-fb3a50c29735",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_sim = genSimGraph(coll_dir, 'sim_op.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f8446779-c9ed-4193-ab5c-35436eb164de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Term set for docs\n",
      "For Folder: sci.space\n",
      "file: sci.space/61444 file: sci.space/61440 file: sci.space/61396 file: sci.space/61363 file: sci.space/61321 file: sci.space/61414 file: sci.space/62114 file: sci.space/62405 file: sci.space/61522 file: sci.space/61416 10\n",
      "Read File Count: 10\n"
     ]
    }
   ],
   "source": [
    "doc_term_dict = g_sim.generate_term_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "de9fc910-3579-4518-8c19-d0032e1c2dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.20178042, 0.18181818, 0.16060606, 0.10365854,\n",
       "        0.23611111, 0.13141026, 0.10309278, 0.11552347, 0.09430894],\n",
       "       [0.        , 0.        , 0.17567568, 0.18897638, 0.124     ,\n",
       "        0.15062762, 0.13333333, 0.13207547, 0.13432836, 0.07441016],\n",
       "       [0.        , 0.        , 0.        , 0.22556391, 0.12132353,\n",
       "        0.14122137, 0.12547529, 0.12820513, 0.125     , 0.09608541],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.12608696,\n",
       "        0.17674419, 0.1627907 , 0.14136126, 0.13812155, 0.08778626],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.11538462, 0.10628019, 0.09444444, 0.13496933, 0.09145129],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.10945274, 0.13017751, 0.1474359 , 0.07100592],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.11904762, 0.13548387, 0.06930693],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.152     , 0.04958678],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.06666667],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_sim.jaccard_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ba27c6e2-0d64-4056-871a-bed946aa1fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating vocab\n",
      "For Folder: sci.space\n",
      "file: sci.space/61444 file: sci.space/61440 file: sci.space/61396 file: sci.space/61363 file: sci.space/61321 file: sci.space/61414 file: sci.space/62114 file: sci.space/62405 file: sci.space/61522 file: sci.space/61416 10\n",
      "Creating vector for docs\n",
      "For Folder: sci.space\n",
      "file: sci.space/61444 file: sci.space/61440 file: sci.space/61396 file: sci.space/61363 file: sci.space/61321 file: sci.space/61414 file: sci.space/62114 file: sci.space/62405 file: sci.space/61522 file: sci.space/61416 10\n",
      "file count: 10\n",
      "[[0.         0.97727218 0.96927141 0.97953964 0.97335732 0.98065062\n",
      "  0.97766819 0.97736422 0.97835111 0.9145893 ]\n",
      " [0.         0.         0.97795757 0.98943328 0.98456061 0.98943724\n",
      "  0.98879728 0.98859438 0.99002432 0.92500254]\n",
      " [0.         0.         0.         0.98010821 0.97467118 0.97933804\n",
      "  0.97898665 0.97865866 0.97945428 0.91735388]\n",
      " [0.         0.         0.         0.         0.98762272 0.99283332\n",
      "  0.99184685 0.99191846 0.9931889  0.92910332]\n",
      " [0.         0.         0.         0.         0.         0.98870588\n",
      "  0.98951562 0.98955512 0.99088498 0.92552923]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.99266306 0.99332684 0.99398202 0.9297131 ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.99399226 0.99504232 0.92996977]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.99528365 0.92956473]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.93034895]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_sim.tf_idf_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67af4af3-7124-4b35-b22b-9c25d50e1088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a9864303-b08c-4258-8a23-0cdc354c08d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sknetwork'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_83887/1361074950.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msknetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mranking\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPageRank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sknetwork'"
     ]
    }
   ],
   "source": [
    "from sknetwork.ranking import PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb0d215-2702-4f30-b24a-27c42bb19a18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cec0e1-2a63-4909-b3fb-05a92e82a803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56c84e6-3a94-48d6-a873-1c499fbce82d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9fd320-be44-4c7b-a1e5-5b232f8e9d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d948ee2-7a45-4b56-86dc-8c6758fabbfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e12fbf-ca8e-4b3d-878a-7b739092f155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e78996c-f93a-458d-9c92-f450d7e39cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
